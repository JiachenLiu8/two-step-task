{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "import argparse\n",
    "import datetime\n",
    "import scipy.signal\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm \n",
    "from collections import namedtuple\n",
    "\n",
    "from models.a2c_lstm_my import A2C_LSTM\n",
    "from tasks.two_step_my import TwoStepTask\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#from scipy.special import softmax\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(tau,x):\n",
    "    return np.exp(tau*x)/sum(np.exp(tau*x))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = 0 # 0 is MF, 1 is MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rollout = namedtuple('Rollout',\n",
    "                        ('state', 'action', 'reward', 'timestep', 'done', 'policy', 'value'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer: \n",
    "    def __init__(self, config, plot_param):\n",
    "        print(config['run-title'])\n",
    "\n",
    "        self.plot_param = plot_param\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        self.seed = config[\"seed\"]\n",
    "\n",
    "        T.manual_seed(config[\"seed\"])\n",
    "        np.random.seed(config[\"seed\"])\n",
    "        T.random.manual_seed(config[\"seed\"])\n",
    "\n",
    "        self.env = TwoStepTask(config[\"task\"])  \n",
    "   \n",
    "\n",
    "        \n",
    "        self.max_grad_norm = config[\"a2c\"][\"max-grad-norm\"]\n",
    "        self.switch_p = config[\"task\"][\"switch-prob\"]\n",
    "        self.start_episode = 0\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(\"logs\", config[\"run-title\"],config[\"run-title\"]+str(self.plot_param)))\n",
    "        self.save_path = os.path.join(config[\"save-path\"], config[\"run-title\"], config[\"run-title\"]+str(self.plot_param))\n",
    "        \n",
    "\n",
    "\n",
    "    def run_episode(self, episode, det=False, params = [0.1,0.1]):\n",
    "\n",
    "        tau = 1\n",
    "        epsilon = params[0]\n",
    "        lrate = params[1]\n",
    "        eta = 0.1\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_entropy = 0\n",
    "\n",
    "\n",
    "        p_action, p_reward, timestep = [0,0], 0, 0\n",
    "\n",
    "        state = self.env.reset()\n",
    "        \n",
    "\n",
    "        Q_s0 = np.zeros(2)\n",
    "        Q_stage2 = np.zeros(2)\n",
    "        p_hat = np.zeros([2,2])\n",
    "\n",
    "        \n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            counter += 1\n",
    "            # switch reward contingencies at the beginning of each episode with probability p\n",
    "            #self.env.possible_switch(switch_p=self.switch_p)\n",
    "\n",
    "            # sample action using model\n",
    "\n",
    "            epsilon = epsilon * 0.99\n",
    "           \n",
    "            action_dist = softmax(tau,Q_s0)\n",
    "\n",
    "            action_dist = T.from_numpy(action_dist)\n",
    "\n",
    "            action_cat = T.distributions.Categorical(action_dist)\n",
    "\n",
    "            if counter >300:\n",
    "                action = np.argmax(action_dist).item()\n",
    "            else:\n",
    "                # use epsilon greedy\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action =  np.random.choice([0, 1])\n",
    "                else:\n",
    "                    action = np.argmax(action_dist).item()    \n",
    "\n",
    "\n",
    "           \n",
    "             \n",
    "            action_onehot = np.eye(2)[action]\n",
    "\n",
    "            # take action and observe result\n",
    "            new_state, reward, done, timestep = self.env.step(int(action))\n",
    "       \n",
    "            # true planet arrived\n",
    "            planet_arrived = np.where(new_state) \n",
    "            planet_arrived = int(planet_arrived[0]) # 1 or 2\n",
    "            planet_arrived = planet_arrived - 1\n",
    "\n",
    "            \n",
    "            gamma = 1\n",
    "\n",
    "\n",
    "            # update Q value\n",
    "\n",
    "            if strategy == 0: # MF  \n",
    "                Q_stage2[planet_arrived] = Q_stage2[planet_arrived] + lrate*(reward-Q_stage2[planet_arrived])\n",
    "            \n",
    "                Q_s0[action] = Q_s0[action] + lrate*(gamma*Q_stage2[planet_arrived] - Q_s0[action])\n",
    "\n",
    "            else: #MB\n",
    "                p_hat[action,planet_arrived] = p_hat[action,planet_arrived]+eta*(1-p_hat[action,planet_arrived])\n",
    "                p_hat_temp = p_hat\n",
    "                p_hat[action,:]= p_hat_temp[action,:]/sum(p_hat_temp[action,:])\n",
    "\n",
    "                Q_stage2[planet_arrived] = Q_stage2[planet_arrived] + lrate*(reward-Q_stage2[planet_arrived])\n",
    "            \n",
    "            \n",
    "                Q_s0[action ] = gamma*np.sum(Q_stage2*p_hat[action,:])\n",
    "                Q_s0[1-action] = gamma*np.sum(Q_stage2*p_hat[1-action,:])\n",
    "                #Q_s0[action] = Q_s0[action] + lrate*(gamma*np.sum(Q_stage2*p_hat[action,:]) - Q_s0[action]) \n",
    "                #Q_s0[1-action] = Q_s0[1-action] + lrate*(gamma*np.sum(Q_stage2*p_hat[1-action,:]) - Q_s0[1-action]) \n",
    "\n",
    "            \n",
    "                \n",
    "            #prev_state = state\n",
    "            state = new_state\n",
    "            p_reward = reward\n",
    "            p_action = action_onehot\n",
    "\n",
    "            \n",
    "            if counter > 300:\n",
    "                total_reward += reward\n",
    "                px = softmax(tau,Q_s0)\n",
    "                logpx = np.log(action_dist)\n",
    "                total_entropy = total_entropy + px[0]*logpx[0] + px[1]*logpx[1]\n",
    "                \n",
    "  \n",
    "     \n",
    "                \n",
    "        total_entropy = -total_entropy\n",
    "\n",
    "        return total_reward, total_entropy\n",
    "    \n",
    "\n",
    "    def train(self,params, max_episodes, save_interval):\n",
    "       \n",
    "\n",
    "        total_rewards = np.zeros(max_episodes)\n",
    "        progress = tqdm(range(self.start_episode, max_episodes))\n",
    "\n",
    "        for episode in progress:\n",
    "\n",
    "            reward = self.run_episode(episode=episode,det=False,params=params)\n",
    "       \n",
    "          \n",
    "            total_rewards[episode] = reward\n",
    "\n",
    "            avg_reward_10 = total_rewards[max(0, episode-10):(episode+1)].mean()\n",
    "            avg_reward_100 = total_rewards[max(0, episode-100):(episode+1)].mean()\n",
    "            self.writer.add_scalar(\"perf/reward_t\", reward, episode)\n",
    "            self.writer.add_scalar(\"perf/avg_reward_10\", avg_reward_10, episode)\n",
    "            self.writer.add_scalar(\"perf/avg_reward_100\", avg_reward_100, episode)\n",
    "            \n",
    "            \n",
    "            #progress.set_description(f\"Episode {episode}/{max_episodes} | Reward: {reward} | Last 10: {avg_reward_10:.4f} \")\n",
    "        return -sum(total_rewards)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def test(self, num_episodes,optimal_params):\n",
    "        progress = tqdm(range(num_episodes))\n",
    "        self.env.reset_transition_count()\n",
    "        \n",
    "\n",
    "        total_rewards = np.zeros(num_episodes)\n",
    "        total_entropies = np.zeros(num_episodes)\n",
    " \n",
    "        for episode in progress:\n",
    "            reward, entropy = self.run_episode(episode, det=False, params = optimal_params)\n",
    "\n",
    "            total_rewards[episode] = reward\n",
    "            total_entropies[episode] = entropy\n",
    "\n",
    "            avg_reward = total_rewards[max(0, episode-10):(episode+1)].mean()            \n",
    "            #progress.set_description(f\"Episode {episode}/{num_episodes} | Reward: {reward} | Last 10: {avg_reward:.4f}\")\n",
    "   \n",
    "        \n",
    "        reward_fraction = sum(total_rewards)/(0.5*(len(total_rewards)*config['task'][\"trials-per-epi\"]))\n",
    "        print(\"fraction of rewarded trials:\", reward_fraction)\n",
    "        #self.env.plot(self.save_path)\n",
    "\n",
    "\n",
    "        avg_entropy = sum(total_entropies)/(0.5*(len(total_entropies)*config['task'][\"trials-per-epi\"]))\n",
    "        return reward_fraction, avg_entropy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if not os.path.isdir('./temp_results'): \n",
    "        os.makedirs('./temp_results')\n",
    "\n",
    "\n",
    "    yaml_path = \"./configs/two_step_my.yaml\"\n",
    "    with open(yaml_path, 'r', encoding=\"utf-8\") as fin:\n",
    "        config = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "\n",
    "    n_seeds = 1\n",
    "    base_seed = config[\"seed\"]\n",
    "    base_run_title = config[\"run-title\"]\n",
    "    buffer_results = {}\n",
    "    test_common_probs = [config['task']['common-prob']]\n",
    "    for common_prob in test_common_probs:\n",
    "\n",
    "        buffer_results[common_prob]={}\n",
    "    \n",
    "        config['task']['common-prob']=common_prob\n",
    "        for seed_idx in range(1, n_seeds + 1):\n",
    "            \n",
    "            config[\"seed\"] = base_seed * seed_idx\n",
    "            config[\"run-title\"] = base_run_title + '_seed='+str(config['seed'])    \n",
    "\n",
    "            exp_path = os.path.join(config[\"save-path\"], config[\"run-title\"])\n",
    "            if not os.path.isdir(exp_path): \n",
    "                os.makedirs(exp_path)\n",
    "            \n",
    "            out_path = os.path.join(exp_path, os.path.basename(yaml_path))\n",
    "            with open(out_path, 'w') as fout:\n",
    "                yaml.dump(config, fout)\n",
    "\n",
    "            plot_param = {'seed':config['seed'],'common_prob':common_prob,'safebet':config[\"task\"]['safebet-reward']}\n",
    "\n",
    "            print(f\"> Running {config['run-title']}\")\n",
    "            trainer = Trainer(config,plot_param)\n",
    "            if config[\"train\"]:\n",
    "                #result = trainer.train(params= [0.1,0.9],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]))\n",
    "                result = minimize(fun=trainer.train, x0 = [0.1,0.8],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]),bounds = ((0,1),(0,1)))\n",
    "                optimal_params = result.x\n",
    "\n",
    "            optimal_params = [0.1,0.8]\n",
    "                \n",
    "            if config[\"test\"]:\n",
    "                reward_fraction,avg_entropy = trainer.test(config[\"task\"][\"test-episodes\"],optimal_params)\n",
    "                # total_buffer=pd.DataFrame(total_buffer, columns=Rollout._fields)\n",
    "                # buffer_results[common_prob][seed_idx-1] = total_buffer\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(ambiguity,seed):\n",
    "    \n",
    "    if not os.path.isdir('./temp_results'): \n",
    "        os.makedirs('./temp_results')\n",
    "\n",
    "\n",
    "    yaml_path = \"./configs/two_step_my.yaml\"\n",
    "    with open(yaml_path, 'r', encoding=\"utf-8\") as fin:\n",
    "        config = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "\n",
    "    config['task']['ambiguity'] = ambiguity\n",
    "    config['seed'] = seed\n",
    "\n",
    "    n_seeds = 1\n",
    "    base_seed = config[\"seed\"]\n",
    "    base_run_title = config[\"run-title\"]\n",
    "    buffer_results = {}\n",
    "    test_common_probs = [config['task']['common-prob']]\n",
    "    for common_prob in test_common_probs:\n",
    "\n",
    "        buffer_results[common_prob]={}\n",
    "    \n",
    "        config['task']['common-prob']=common_prob\n",
    "        for seed_idx in range(1, n_seeds + 1):\n",
    "            \n",
    "            config[\"seed\"] = base_seed * seed_idx\n",
    "            config[\"run-title\"] = base_run_title + '_seed='+str(config['seed'])    \n",
    "\n",
    "            exp_path = os.path.join(config[\"save-path\"], config[\"run-title\"])\n",
    "            if not os.path.isdir(exp_path): \n",
    "                os.makedirs(exp_path)\n",
    "            \n",
    "            out_path = os.path.join(exp_path, os.path.basename(yaml_path))\n",
    "            with open(out_path, 'w') as fout:\n",
    "                yaml.dump(config, fout)\n",
    "\n",
    "            plot_param = {'seed':config['seed'],'common_prob':common_prob,'safebet':config[\"task\"]['safebet-reward']}\n",
    "\n",
    "            print(f\"> Running {config['run-title']}\")\n",
    "            trainer = Trainer(config,plot_param)\n",
    "            if config[\"train\"]:\n",
    "                #result = trainer.train(params= [0.1,0.9],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]))\n",
    "                result = minimize(fun=trainer.train, x0 = [0.1,0.8],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]),bounds = ((0,1),(0,1)))\n",
    "                optimal_params = result.x\n",
    "\n",
    "            optimal_params = [0.1,0.8]\n",
    "                \n",
    "            if config[\"test\"]:\n",
    "                reward_fraction, entropy = trainer.test(config[\"task\"][\"test-episodes\"],optimal_params)\n",
    "                # total_buffer=pd.DataFrame(total_buffer, columns=Rollout._fields)\n",
    "                # buffer_results[common_prob][seed_idx-1] = total_buffer\n",
    "\n",
    "    return reward_fraction, entropy\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance comparison of Q1 and MB across ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = 0\n",
    "n_rounds = 4\n",
    "MF_results = np.zeros([n_rounds,len(np.arange(0,1.1,0.1))])\n",
    "ambiguities = np.arange(0,1.1,0.1)\n",
    "for iter in range(n_rounds):\n",
    "    seed = 100*iter\n",
    "    for ambiguity_idx in range(len(ambiguities)):\n",
    "        ambiguity = ambiguities[ambiguity_idx]\n",
    "        reward_fraction,_ = run(ambiguity,seed)\n",
    "        MF_results[iter,ambiguity_idx] = reward_fraction\n",
    "MF_avg = np.mean(MF_results,axis=0)\n",
    "sems_MF = sem(MF_results)\n",
    "\n",
    "strategy = 1\n",
    "\n",
    "MB_results = np.zeros([n_rounds,len(np.arange(0,1.1,0.1))])\n",
    "ambiguities = np.arange(0,1.1,0.1)\n",
    "for iter in range(n_rounds):\n",
    "    seed = 100*iter\n",
    "    for ambiguity_idx in range(len(ambiguities)):\n",
    "        ambiguity = ambiguities[ambiguity_idx]\n",
    "        reward_fraction, _ = run(ambiguity,seed)\n",
    "        MB_results[iter,ambiguity_idx] = reward_fraction\n",
    "MB_avg = np.mean(MB_results,axis=0)\n",
    "sems_MB = sem(MB_results)\n",
    "\n",
    "\n",
    "plt.plot(MF_avg,color = '#1f77b4',alpha=0.8)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= np.arange(0,1.1,0.1),yerr=sems_MF,color='#1f77b4',alpha=0.1,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot(MB_avg,color = '#1f77b4',alpha=0.8)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= np.arange(0,1.1,0.1),yerr=sems_MB,color='#ff7f0e',alpha=0.1,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,1.1,0.1),MF_avg,color = '#AA041F',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= np.arange(0,1.1,0.1),yerr=sems_MF,color='#AA041F',alpha=0.8,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot(np.arange(0,1.1,0.1),MB_avg,color = '#0B224A',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= np.arange(0,1.1,0.1),yerr=sems_MB,color='#0B224A',alpha=0.8,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])\n",
    "plt.xlabel(\"ambiguity\")\n",
    "plt.ylabel(\"fraction of rewarded trials\")\n",
    "plt.title('agent performance across different ambiguities')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance comparison of Q1 and MB across common transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(p_common,seed):\n",
    "    \n",
    "    if not os.path.isdir('./temp_results'): \n",
    "        os.makedirs('./temp_results')\n",
    "\n",
    "\n",
    "    yaml_path = \"./configs/two_step_my.yaml\"\n",
    "    with open(yaml_path, 'r', encoding=\"utf-8\") as fin:\n",
    "        config = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "\n",
    "    config['task']['common-prob'] = p_common\n",
    "    config['seed'] = seed\n",
    "\n",
    "    n_seeds = 1\n",
    "    base_seed = config[\"seed\"]\n",
    "    base_run_title = config[\"run-title\"]\n",
    "    buffer_results = {}\n",
    "    test_common_probs = [config['task']['common-prob']]\n",
    "    for common_prob in test_common_probs:\n",
    "\n",
    "        buffer_results[common_prob]={}\n",
    "    \n",
    "        config['task']['common-prob']=common_prob\n",
    "        for seed_idx in range(1, n_seeds + 1):\n",
    "            \n",
    "            config[\"seed\"] = base_seed * seed_idx\n",
    "            config[\"run-title\"] = base_run_title + '_seed='+str(config['seed'])    \n",
    "\n",
    "            exp_path = os.path.join(config[\"save-path\"], config[\"run-title\"])\n",
    "            if not os.path.isdir(exp_path): \n",
    "                os.makedirs(exp_path)\n",
    "            \n",
    "            out_path = os.path.join(exp_path, os.path.basename(yaml_path))\n",
    "            with open(out_path, 'w') as fout:\n",
    "                yaml.dump(config, fout)\n",
    "\n",
    "            plot_param = {'seed':config['seed'],'common_prob':common_prob,'safebet':config[\"task\"]['safebet-reward']}\n",
    "\n",
    "            print(f\"> Running {config['run-title']}\")\n",
    "            trainer = Trainer(config,plot_param)\n",
    "            if config[\"train\"]:\n",
    "                #result = trainer.train(params= [0.1,0.9],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]))\n",
    "                result = minimize(fun=trainer.train, x0 = [0.1,0.8],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]),bounds = ((0,1),(0,1)))\n",
    "                optimal_params = result.x\n",
    "\n",
    "            optimal_params = [0.1,0.8]\n",
    "                \n",
    "            if config[\"test\"]:\n",
    "                reward_fraction, entropy = trainer.test(config[\"task\"][\"test-episodes\"],optimal_params)\n",
    "                # total_buffer=pd.DataFrame(total_buffer, columns=Rollout._fields)\n",
    "                # buffer_results[common_prob][seed_idx-1] = total_buffer\n",
    "\n",
    "    return reward_fraction, entropy\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = 0\n",
    "n_rounds = 4\n",
    "MF_results = np.zeros([n_rounds,len([0.5,0.6,0.7,0.8,0.9,1])])\n",
    "p_commons = [0.5,0.6,0.7,0.8,0.9,1]\n",
    "for iter in range(n_rounds):\n",
    "    seed = 100*iter\n",
    "    for p_common_idx in range(len(p_commons)):\n",
    "        p_common = p_commons[p_common_idx]\n",
    "        reward_fraction,_ = run(p_common,seed)\n",
    "        MF_results[iter,p_common_idx] = reward_fraction\n",
    "MF_avg = np.mean(MF_results,axis=0)\n",
    "sems_MF = sem(MF_results)\n",
    "\n",
    "strategy = 1\n",
    "\n",
    "MB_results = np.zeros([n_rounds,len([0.5,0.6,0.7,0.8,0.9,1])])\n",
    "\n",
    "for iter in range(n_rounds):\n",
    "    seed = 100*iter\n",
    "    for p_common_idx in range(len(p_commons)):\n",
    "        p_common = p_commons[p_common_idx]\n",
    "        reward_fraction, _ = run(p_common,seed)\n",
    "        MB_results[iter,p_common_idx] = reward_fraction\n",
    "MB_avg = np.mean(MB_results,axis=0)\n",
    "sems_MB = sem(MB_results)\n",
    "\n",
    "\n",
    "plt.plot([0.5,0.6,0.7,0.8,0.9,1],MF_avg,color = '#1f77b4',alpha=0.8)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= [0.5,0.6,0.7,0.8,0.9,1],yerr=sems_MF,color='#1f77b4',alpha=0.1,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot([0.5,0.6,0.7,0.8,0.9,1],MB_avg,color = '#1f77b4',alpha=0.8)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= [0.5,0.6,0.7,0.8,0.9,1],yerr=sems_MB,color='#ff7f0e',alpha=0.1,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0.5,0.6,0.7,0.8,0.9,1],MF_avg,color = '#AA041F',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= [0.5,0.6,0.7,0.8,0.9,1],yerr=sems_MF,color='#AA041F',alpha=0.8,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot([0.5,0.6,0.7,0.8,0.9,1],MB_avg,color = '#0B224A',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= [0.5,0.6,0.7,0.8,0.9,1],yerr=sems_MB,color='#0B224A',alpha=0.8,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])\n",
    "plt.xlabel(\"p_common\")\n",
    "plt.ylabel(\"fraction of rewarded trials\")\n",
    "plt.title('agent performance across different common transitions')\n",
    "plt.xticks([0.5,0.6,0.7,0.8,0.9,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subjective confidence comparison between Q1 and MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = 0\n",
    "n_rounds = 4\n",
    "MF_results = np.zeros([n_rounds,len(np.arange(0,1.1,0.1))])\n",
    "ambiguities = np.arange(0,1.1,0.1)\n",
    "for iter in range(n_rounds):\n",
    "    seed = 100*iter\n",
    "    for ambiguity_idx in range(len(ambiguities)):\n",
    "        ambiguity = ambiguities[ambiguity_idx]\n",
    "        _, entropy = run(ambiguity,seed)\n",
    "        MF_results[iter,ambiguity_idx] = entropy\n",
    "MF_avg = np.mean(MF_results,axis=0)\n",
    "sems_MF = sem(MF_results)\n",
    "\n",
    "strategy = 1\n",
    "\n",
    "MB_results = np.zeros([n_rounds,len(np.arange(0,1.1,0.1))])\n",
    "ambiguities = np.arange(0,1.1,0.1)\n",
    "for iter in range(n_rounds):\n",
    "    seed = 100*iter\n",
    "    for ambiguity_idx in range(len(ambiguities)):\n",
    "        ambiguity = ambiguities[ambiguity_idx]\n",
    "        _,entropy = run(ambiguity,seed)\n",
    "        MB_results[iter,ambiguity_idx] = entropy\n",
    "MB_avg = np.mean(MB_results,axis=0)\n",
    "sems_MB = sem(MB_results)\n",
    "\n",
    "\n",
    "plt.plot(MF_avg,color = '#1f77b4',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= np.arange(0,1.1,0.1),yerr=sems_MF,color='#1f77b4',alpha=0.8,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot(MB_avg,color = '#1f77b4',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= np.arange(0,1.1,0.1),yerr=sems_MB,color='#ff7f0e',alpha=0.8,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,1.1,0.1),MF_avg,color = '#1f77b4',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= np.arange(0,1.1,0.1),yerr=sems_MF,color='#1f77b4',alpha=0.8,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot(np.arange(0,1.1,0.1),MB_avg,color = '#ff7f0e',alpha=1)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= np.arange(0,1.1,0.1),yerr=sems_MB,color='#ff7f0e',alpha=0.8,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])\n",
    "plt.xlabel(\"ambiguity\")\n",
    "plt.ylabel(\"entropy\")\n",
    "plt.title('agent entropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,1.1,0.1),MF_avg,color = '#AA041F',alpha=0.8)\n",
    "\n",
    "plt.errorbar(y=MF_avg,x= np.arange(0,1.1,0.1),yerr=sems_MF,color='#AA041F',alpha=0.1,capsize=3)\n",
    "      \n",
    "\n",
    "plt.plot(np.arange(0,1.1,0.1),MB_avg,color = '#0B224A',alpha=0.8)\n",
    "\n",
    "plt.errorbar(y=MB_avg,x= np.arange(0,1.1,0.1),yerr=sems_MB,color='#0B224A',alpha=0.1,capsize=3)\n",
    "\n",
    "plt.legend([\"MF\",\"MB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rollout = namedtuple('Rollout',\n",
    "                        ('state', 'action', 'reward', 'timestep', 'done', 'policy', 'value'))\n",
    "\n",
    "\n",
    "class Trainer: \n",
    "    def __init__(self, config, plot_param):\n",
    "        print(config['run-title'])\n",
    "\n",
    "        self.plot_param = plot_param\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        self.seed = config[\"seed\"]\n",
    "\n",
    "        T.manual_seed(config[\"seed\"])\n",
    "        np.random.seed(config[\"seed\"])\n",
    "        T.random.manual_seed(config[\"seed\"])\n",
    "\n",
    "        self.env = TwoStepTask(config[\"task\"])  \n",
    "   \n",
    "\n",
    "        \n",
    "        self.max_grad_norm = config[\"a2c\"][\"max-grad-norm\"]\n",
    "        self.switch_p = config[\"task\"][\"switch-prob\"]\n",
    "        self.start_episode = 0\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(\"logs\", config[\"run-title\"],config[\"run-title\"]+str(self.plot_param)))\n",
    "        self.save_path = os.path.join(config[\"save-path\"], config[\"run-title\"], config[\"run-title\"]+str(self.plot_param))\n",
    "        \n",
    "\n",
    "\n",
    "    def run_episode(self, episode, det=False , params = None):\n",
    "\n",
    "        epsilon = params[0]\n",
    "        #tau = params[0]\n",
    "        eta = params[1]\n",
    "        lrate = params[2]\n",
    "\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "\n",
    "        p_action, p_reward, timestep = [0,0], 0, 0\n",
    "\n",
    "        state = self.env.reset()\n",
    "        \n",
    "\n",
    "        Q_s0 = np.zeros(2)\n",
    "        Q_stage2 = np.zeros(2)\n",
    "        avg_reward_stage2 = np.zeros(2)\n",
    "        p_hat = np.zeros([2,2]) # first row is prob of state 1 and 2 for action 1, second row is for action 2\n",
    "\n",
    "        counter = 0\n",
    "        while not done:\n",
    "\n",
    "            counter = counter+1\n",
    "\n",
    "            # switch reward contingencies at the beginning of each episode with probability p\n",
    "            #self.env.possible_switch(switch_p=self.switch_p)\n",
    "\n",
    "            # sample action using model\n",
    "           \n",
    "            action_dist = softmax(1,Q_s0)\n",
    "\n",
    "            action_dist = T.from_numpy(action_dist)\n",
    "\n",
    "            \n",
    "      \n",
    "\n",
    "            # use epsilon greedy\n",
    "            if counter >300:\n",
    "                action = np.argmax(action_dist).item()\n",
    "            else:\n",
    "                # use epsilon greedy\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action =  np.random.choice([0, 1])\n",
    "                else:\n",
    "                    action = np.argmax(action_dist).item()   \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "             \n",
    "            action_onehot = np.eye(2)[action]\n",
    "\n",
    "            # take action and observe result\n",
    "            new_state, reward, done, timestep = self.env.step(int(action))\n",
    "       \n",
    "            # true planet arrived\n",
    "            planet_arrived = np.where(new_state) \n",
    "            planet_arrived = int(planet_arrived[0]) # 1 or 2\n",
    "            planet_arrived = planet_arrived - 1\n",
    "\n",
    "          \n",
    "            # eta = 0.5\n",
    "            # lrate = 0.6\n",
    "            gamma = 1\n",
    "\n",
    "\n",
    "            p_hat[action,planet_arrived] = p_hat[action,planet_arrived]+eta*(1-p_hat[action,planet_arrived])\n",
    "            p_hat_temp = p_hat\n",
    "            p_hat[action,:]= p_hat_temp[action,:]/sum(p_hat_temp[action,:])\n",
    "\n",
    "            Q_stage2[planet_arrived] = Q_stage2[planet_arrived] + lrate*(reward-Q_stage2[planet_arrived])\n",
    "           \n",
    "           \n",
    "            Q_s0[action ] = gamma*np.sum(Q_stage2*p_hat[action,:])\n",
    "            Q_s0[1-action] = gamma*np.sum(Q_stage2*p_hat[1-action,:])\n",
    "            # Q_s0[action] = Q_s0[action] + lrate*(gamma*np.sum(Q_stage2*p_hat[action,:]) - Q_s0[action]) \n",
    "            # Q_s0[1-action] = Q_s0[1-action] + lrate*(gamma*np.sum(Q_stage2*p_hat[1-action,:]) - Q_s0[1-action])   \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            #prev_state = state\n",
    "            state = new_state\n",
    "            p_reward = reward\n",
    "            p_action = action_onehot\n",
    "            \n",
    "            if counter > 300:\n",
    "                total_reward += reward\n",
    "\n",
    "\n",
    "        return total_reward\n",
    "    \n",
    "\n",
    "    def train(self, params, max_episodes, save_interval):\n",
    "\n",
    "        total_rewards = np.zeros(max_episodes)\n",
    "        progress = tqdm(range(self.start_episode, max_episodes))\n",
    "\n",
    "        for episode in progress:\n",
    "\n",
    "            reward = self.run_episode(episode=episode, det=False,params=params)\n",
    "       \n",
    "          \n",
    "            total_rewards[episode] = reward\n",
    "\n",
    "            avg_reward_10 = total_rewards[max(0, episode-10):(episode+1)].mean()\n",
    "            avg_reward_100 = total_rewards[max(0, episode-100):(episode+1)].mean()\n",
    "            self.writer.add_scalar(\"perf/reward_t\", reward, episode)\n",
    "            self.writer.add_scalar(\"perf/avg_reward_10\", avg_reward_10, episode)\n",
    "            self.writer.add_scalar(\"perf/avg_reward_100\", avg_reward_100, episode)\n",
    "            \n",
    "            \n",
    "           # progress.set_description(f\"Episode {episode}/{max_episodes} | Reward: {reward} | Last 10: {avg_reward_10:.4f} \")\n",
    "        \n",
    "       \n",
    "        return -sum(total_rewards)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def test(self, num_episodes,optimal_params):\n",
    "        progress = tqdm(range(num_episodes))\n",
    "        self.env.reset_transition_count()\n",
    "        \n",
    "        total_rewards = np.zeros(num_episodes)\n",
    "\n",
    "        for episode in progress:\n",
    "            reward = self.run_episode(episode, det=False, params=optimal_params)\n",
    "\n",
    "            total_rewards[episode] = reward\n",
    "            avg_reward = total_rewards[max(0, episode-10):(episode+1)].mean()            \n",
    "            progress.set_description(f\"Episode {episode}/{num_episodes} | Reward: {reward} | Last 10: {avg_reward:.4f}\")\n",
    "\n",
    "        reward_fraction = sum(total_rewards)/(0.5*(len(total_rewards)*config['task'][\"trials-per-epi\"]))\n",
    "\n",
    "        print(\"fraction of rewarded trials:\", reward_fraction)\n",
    "        self.env.plot(self.save_path)\n",
    "        \n",
    "              \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if not os.path.isdir('./temp_results'): \n",
    "        os.makedirs('./temp_results')\n",
    "\n",
    "\n",
    "    yaml_path = \"./configs/two_step_my.yaml\"\n",
    "    with open(yaml_path, 'r', encoding=\"utf-8\") as fin:\n",
    "        config = yaml.load(fin, Loader=yaml.FullLoader)\n",
    "\n",
    "    n_seeds = 1\n",
    "    base_seed = config[\"seed\"]\n",
    "    base_run_title = config[\"run-title\"]\n",
    "    buffer_results = {}\n",
    "    test_common_probs = [config['task']['common-prob']]\n",
    "    for common_prob in test_common_probs:\n",
    "\n",
    "        buffer_results[common_prob]={}\n",
    "    \n",
    "        config['task']['common-prob']=common_prob\n",
    "        for seed_idx in range(1, n_seeds + 1):\n",
    "            \n",
    "            config[\"seed\"] = base_seed * seed_idx\n",
    "            config[\"run-title\"] = base_run_title + '_seed='+str(config['seed'])    \n",
    "\n",
    "            exp_path = os.path.join(config[\"save-path\"], config[\"run-title\"])\n",
    "            if not os.path.isdir(exp_path): \n",
    "                os.makedirs(exp_path)\n",
    "            \n",
    "            out_path = os.path.join(exp_path, os.path.basename(yaml_path))\n",
    "            with open(out_path, 'w') as fout:\n",
    "                yaml.dump(config, fout)\n",
    "\n",
    "            plot_param = {'seed':config['seed'],'common_prob':common_prob,'safebet':config[\"task\"]['safebet-reward']}\n",
    "\n",
    "            print(f\"> Running {config['run-title']}\")\n",
    "            trainer = Trainer(config,plot_param)\n",
    "            if config[\"train\"]:\n",
    "                result = minimize(fun=trainer.train, x0 = [0.1,0.2,0.2],args=(config[\"task\"][\"train-episodes\"],  config[\"save-interval\"]),bounds=((0,1),(0,1),(0,1)))\n",
    "                optimal_params = result.x\n",
    "\n",
    "            optimal_params = [0.1,0.2,0.2]\n",
    "\n",
    "            if config[\"test\"]:\n",
    "                total_buffer = trainer.test(config[\"task\"][\"test-episodes\"],optimal_params)\n",
    "                total_buffer=pd.DataFrame(total_buffer, columns=Rollout._fields)\n",
    "                buffer_results[common_prob][seed_idx-1] = total_buffer\n",
    "            print(\"optimal_params\",optimal_params)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_arm64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
